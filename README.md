ğŸ©º Diabetes Prediction using Machine Learning

ğŸ“– Project Overview

This project applies machine learning techniques to predict the likelihood of diabetes in patients based on health measurements such as glucose level, BMI, and age.

The dataset used is the Pima Indians Diabetes Dataset, available on Kaggle.

The goal is to compare different machine learning models and evaluate their performance in predicting diabetes, while also highlighting the importance of appropriate evaluation metrics for medical problems.

â¸»

ğŸ“‚ Project Workflow
	1.	Data Exploration (EDA)
	â€¢	Checked dataset distribution, missing values, and correlations.
	â€¢	Visualized class imbalance (diabetic vs. non-diabetic cases).
	2.	Data Preprocessing
	â€¢	Replaced invalid zero values (in columns like Blood Pressure, Insulin, Skin Thickness).
	â€¢	Scaled features using StandardScaler (important for Logistic Regression).
	3.	Model Training
	â€¢	Logistic Regression: Achieved ~75% accuracy.
	â€¢	Random Forest Classifier: Achieved ~72% accuracy with default parameters.
	4.	Model Evaluation
	â€¢	Compared performance using Accuracy, Precision, Recall, F1-score, and ROC-AUC.
	â€¢	Highlighted that in medical diagnosis, Recall (Sensitivity) is often more critical than raw accuracy.

â¸»

ğŸ› ï¸ Technologies Used
	â€¢	Python 3.9+
	â€¢	Google Colab (development environment)
	â€¢	Libraries:
	â€¢	Pandas, NumPy (data handling)
	â€¢	Matplotlib, Seaborn (visualization)
	â€¢	Scikit-learn (ML models & evaluation)

ğŸš€ How to Run
	1.	Clone the repository:
 git clone https://github.com/beetwin1/diabetes-prediction.git 
cd diabetes-prediction
  2.	Install dependencies:
pip install -r requirements.txt
  3.	Open the Jupyter notebook (or Colab):
jupyter notebook notebooks/diabetes_prediction.ipynb

ğŸ“Š Results
	â€¢	Logistic Regression: ~75% accuracy, strong baseline model.
	â€¢	Random Forest Classifier: ~72% accuracy (default parameters).
	â€¢	Key insight: Logistic Regression performed slightly better, but further tuning of Random Forest could improve results.

 ğŸ”® Next Steps
	â€¢	Hyperparameter tuning for Random Forest (GridSearchCV / RandomizedSearchCV).
	â€¢	Test additional algorithms (XGBoost, LightGBM, SVM).
	â€¢	Address class imbalance with SMOTE or class-weight adjustments.
	â€¢	Build a simple web app (Streamlit/Flask) for predictions.

 ğŸ“œ License

This project is released under the MIT License.



ğŸ™Œ Acknowledgements
	â€¢	Dataset: Kaggle â€“ Pima Indians Diabetes Dataset
	â€¢	Libraries: Scikit-learn, Pandas, NumPy, Matplotlib, Seaborn.
